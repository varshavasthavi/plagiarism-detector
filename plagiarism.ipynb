{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import nltk\n",
    "nltk.download()\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#wordnet=WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "import string\n",
    "def preprocess(f,req=0):\n",
    "    filename = f\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    # split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    #print(sentences[0])\n",
    "    #split into words\n",
    "    w=word_tokenize(text)\n",
    "    #print(w[:10])\n",
    "    #removing puctuations and stopwords\n",
    "\n",
    "    stop=stopwords.words(\"english\")\n",
    "    words=[]\n",
    "    for i in w:\n",
    "        if (i not in string.punctuation) and (i not in stop):\n",
    "            #k=wordnet.lemmatize(i)\n",
    "            k=porter.stem(i)\n",
    "            #print(k)\n",
    "            #if k!=i:\n",
    "               # print(k,\"-\",i)\n",
    "            words.append(k)\n",
    "    #print(words[:10])\n",
    "    #print(\"before\",sentences[0])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i]=sentences[i].lower()\n",
    "        for j in sentences[i]:\n",
    "            if j in string.punctuation:\n",
    "                sentences[i]=sentences[i].replace(j,\" \")\n",
    "                \n",
    "        print(sentences[i])\n",
    "    #print(sentences[0])\n",
    "    if req==0:\n",
    "        return(sentences)\n",
    "    else:\n",
    "        k=(len(min(words,key=len))+len(max(words,key=len)))//2\n",
    "        str=\"\"\n",
    "        for i in words:\n",
    "            str=str+i\n",
    "        #print(str)\n",
    "        for i in string.punctuation:\n",
    "            str=str.replace(i,\"\")\n",
    "        return(str,k)\n",
    "#preprocess(\"data1.txt\",0)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingHash:\n",
    "    def __init__(self, text, sizeWord):\n",
    "        self.text = text\n",
    "        self.hash = 0 \n",
    "        self.sizeWord = sizeWord\n",
    "        self.hashtable=[]\n",
    "\n",
    "        for i in range(0, sizeWord):\n",
    "            #ord maps the character to a number\n",
    "            #subtract out the ASCII value of \"a\" to start the indexing at zero\n",
    "            self.hash += (ord(self.text[i]) - ord(\"a\")+1)*(33**(sizeWord - i -1))\n",
    "\n",
    "        #start index of current window\n",
    "        self.window_start = 0\n",
    "        #end of index window\n",
    "        self.window_end = sizeWord\n",
    "\n",
    "    def move_window(self):\n",
    "        if self.window_end <= len(self.text) - 1:\n",
    "            #remove left letter from hash value\n",
    "            self.hash -= (ord(self.text[self.window_start]) - ord(\"a\")+1)*33**(self.sizeWord-1)\n",
    "            self.hash *= 33\n",
    "            self.hash += ord(self.text[self.window_end])- ord(\"a\")+1\n",
    "            self.window_start += 1\n",
    "            self.window_end += 1\n",
    "\n",
    "    def window_text(self):\n",
    "        return self.text[self.window_start:self.window_end]\n",
    "\n",
    "def rabin_karp(word, text):\n",
    "    if word == \"\" or text == \"\":\n",
    "        return None\n",
    "    if len(word) > len(text):\n",
    "        return None\n",
    "\n",
    "    rolling_hash = RollingHash(text, len(word))\n",
    "    #print(rolling_hash)\n",
    "    word_hash = RollingHash(word, len(word))\n",
    "    #print(word_hash)\n",
    "    #word_hash.move_window()\n",
    "\n",
    "    for i in range(len(text) - len(word) + 1):\n",
    "        if rolling_hash.hash == word_hash.hash:\n",
    "            if rolling_hash.window_text() == word:\n",
    "                return i\n",
    "        rolling_hash.move_window()\n",
    "    return None\n",
    "print(rabin_karp('hash','rolling-hash'))\n",
    "def rabin_karp2(text,k=5):\n",
    "    rolling_hash = RollingHash(text,k)\n",
    "    for i in range(len(text) - k + 1):\n",
    "        rolling_hash.hashtable.append(rolling_hash.hash)\n",
    "        rolling_hash.move_window()\n",
    "    return(rolling_hash.hashtable)\n",
    "rabin_karp2(\"aabab\",3)\n",
    "rabin_karp2(\"aaaab\",3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram(file1,file2):\n",
    "    f1=preprocess(file1,1)\n",
    "    #print(\"f1:\",f1)\n",
    "    f2=preprocess(file2,1)\n",
    "    k=f1[1]\n",
    "    f1=f1[0]\n",
    "    f2=f2[0]\n",
    "    print(\"k=\",k)\n",
    "    \n",
    "    #f1=\"plagiarisminstanceusingcloselyimitatinglanguagethoughtsanotherauthorwithoutauthorization\"\n",
    "    #f2=\"plagiarismcopyingideaswordsanotherpersonwithoutgivingcreditthatperson\"\n",
    "    #print(\"f2:\",f2)\n",
    "    table1=rabin_karp2(f1,10)\n",
    "    a=(len(table1))\n",
    "    table2=rabin_karp2(f2,10)\n",
    "    b=(len(table2))\n",
    "    if a<b:\n",
    "        n=table1\n",
    "        m=table2\n",
    "    else:\n",
    "        n=table2\n",
    "        m=table1\n",
    "    val=0\n",
    "    for i in n:\n",
    "        val=val+m.count(i)\n",
    "    #print(val)\n",
    "    return ((2*val)/(a+b))*100\n",
    "    \n",
    "#print(kgram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file1=input(\"enter filename 1:\")\n",
    "    file2=input(\"enter filename 2:\")\n",
    "    sentences=preprocess(file1,0)\n",
    "    f=open(file2,\"r+\")\n",
    "    content=f.read()\n",
    "    content=content.lower()\n",
    "    for i in content:\n",
    "        if i in string.punctuation:\n",
    "            content=content.replace(i,\" \")\n",
    "    #print(content)\n",
    "    \n",
    "\n",
    "    amount=0\n",
    "    start=time.time()\n",
    "    for i in sentences:\n",
    "        temp=rabin_karp(i,content)\n",
    "        if temp!=None:\n",
    "            amount=amount+len(i)\n",
    "    \n",
    "    print(\"percentage of plagiarism using senteces=\",(amount/len(content))*100)\n",
    "    print(\"runtime=\",time.time()-start)\n",
    "    start=time.time()\n",
    "    print(\"percentage of plagiarism using k-grams(words)=\",kgram(file1,file2))\n",
    "    print(\"runtime=\",time.time()-start)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
